# Evolving Trends in the Industry

## What is an Agent?

An **agent** is an autonomous system that can perceive its environment, make decisions, and take actions to achieve specific goals. Unlike traditional software that follows predetermined paths, agents exhibit:

- **Autonomy**: Independent decision-making capabilities
- **Reactivity**: Ability to respond to environmental changes  
- **Proactivity**: Goal-directed behavior
- **Social ability**: Interaction with other agents and humans

```{mermaid}
graph TD
    A[Environment] --> B[Agent]
    B --> C[Perception]
    C --> D[Decision Making]
    D --> E[Action]
    E --> A
    
    style B fill:#e1f5fe
    style D fill:#fff3e0
```

## How Agents Differ from Other AI Systems

Before examining how agents differ from other AI systems, it's important to understand the landscape of AI system architectures and their capabilities.

### Core AI System Architectures

**LLM with Prompt Engineering**: At its simplest, this involves sending a carefully crafted prompt to a language model and receiving a text response. Prompt engineering is the practice of designing and refining these prompts to elicit better responses—adjusting wording, providing examples, specifying output formats, and setting context. The model relies entirely on its training data and the information provided in the prompt.

**RAG (Retrieval-Augmented Generation)**: RAG systems enhance LLMs by first retrieving relevant documents from a knowledge base, then including this retrieved context in the prompt. When a user asks a question, the system searches through indexed documents, selects the most relevant passages, and provides them to the LLM along with the original query. This allows the model to answer questions about information it wasn't trained on. RAG is often implemented using a vector database, since they excel at retrieving relevant unstructured data based on natural language queries.

**Tool-calling Agents**: These systems give LLMs the ability to use external tools and APIs. Instead of just generating text, the agent can decide to call functions, query databases, execute code, or interact with other services. The agent reasons about which tools to use, in what order, and how to combine their outputs to accomplish a task.

**Multi-agent Systems**: These architectures employ multiple specialized agents working together to solve complex problems. Agents might collaborate through various patterns—hierarchical coordination where a supervisor delegates to specialists, peer-to-peer communication where agents directly interact, or competitive approaches where multiple agents propose solutions. Each agent typically specializes in specific tasks or domains.

| Pattern | What it is | Capabilities | Use Cases | Limitations |
|---|---|---|---|---|
| **LLM + prompt** | Single model call with a prompt. | Text generation, reasoning | Q&A, content creation | Static knowledge, no actions |
| **Chain (e.g. basic RAG)** | Fixed, scripted steps (retrieve → augment → generate). | Knowledge retrieval + generation | Document search, knowledge base | Limited to retrieval |
| **Tool-calling agent** | LLM chooses which tool(s) to use at runtime. | Dynamic reasoning + actions | Task automation, workflows | Complexity, reliability |
| **Multi-agent** | Router or coordinator orchestrates specialized sub-agents. | Decomposition, parallelism, role specialization, escalation paths. | Complex workflows and multi-domain support. Support triage across HR, IT, and Finance. | Orchestration overhead. Emergent loops. Hardest to debug and attribute. |


### Ops Maturity Controls by Pattern

As agent architectures grow in complexity, the operational controls required to maintain them must also mature. The table below shows how five critical operational capabilities evolve across different architectural patterns.

1. **Logging & Tracing**: What you need to capture to understand system behavior
2. **Evaluation Gates**: Tests that must pass before deploying changes
3. **Governance & Policy**: Controls on who can change what
4. **Deploy & Rollback**: How you safely release changes
5. **Monitoring & Alerts**: What to watch in production

| Control | LLM + prompt | Chain (basic RAG) | Tool-calling agent | Multi-agent |
|---|---|---|---|---|
| **Logging & tracing** | Single-step trace (prompt I/O, tokens, latency) | Multi-stage traces (query → retrieval → generation) | Per-tool spans (I/O, errors, retries) | Distributed traces (routing, handoffs, shared state) |
| **Evaluation gates** | Golden test set (20-100 examples) | RAG metrics (groundedness, citations) | Task success + side-effects testing | Per-agent + end-to-end workflows |
| **Governance & policy** | Model/prompt change approval | Data source policies and index change control | Role-Based Access Control (RBAC) for tool access | Cross-agent policies and orchestrator gates |
| **Deploy & rollback** | Optional canary, quick rollback | Canary for index/prompt changes | Canary routes + trace replay | Orchestrator-level gates, per-agent versioning |
| **Monitoring & alerts** | Response drift, rate limits | Retrieval quality drift, index freshness | Tool failures/timeouts, quota alerts | Escalation rates, loop detection, per-role costs |

**Logging & tracing** evolves from capturing a single request-response pair to tracking complex multi-agent interactions. Simple LLM calls need only basic I/O logging, but multi-agent systems require distributed tracing that follows requests across agent boundaries, capturing routing decisions and state changes.

**Evaluation gates** grow from basic correctness testing to comprehensive workflow validation. While a golden test set suffices for simple prompts, RAG systems need retrieval-specific metrics like groundedness and citation accuracy. Tool-calling agents require testing both task completion and safety (ensuring tools don't cause unintended side-effects), while multi-agent systems demand end-to-end testing of routing logic and escalation paths. 

A key component of GenAI evaluation is the development of custom evaluation judges. They are LLM-based evaluators that let you define complex and nuanced scoring guidelines for GenAI applications using natural language, enabling organizations to assess outputs against their unique enterprise and use case specific criteria.

Across all patterns, integrating subject matter experts (SMEs) into evaluation is essential for assessing domain-specific correctness that automated metrics cannot capture, such as, legal compliance in document generation or medical accuracy in healthcare applications.

**Governance & policy** progresses from simple change approval to sophisticated access control. Basic systems need prompt versioning, RAG systems add data source governance, tool-calling agents require RBAC to control which tools can be accessed, and multi-agent systems need policies that span multiple agents with comprehensive audit trails.

**Deploy & rollback** capabilities must match system complexity. Simple deployments can use basic canary releases, but as systems grow more complex, you need the ability to replay production traces against new versions, promote through multiple environments, and coordinate versioning across multiple agents.

**Monitoring & alerts** shift from basic health metrics to detecting complex failure modes. Simple systems monitor behavior drift and rate limits, RAG systems add retrieval quality metrics, tool-calling agents track tool execution health, and multi-agent systems must detect emergent problems like infinite loops, where agents repeatedly trigger each other without making progress toward a goal, and escalation bottlenecks, where cases pile up at human review stages faster than they can be processed, indicating the agent's confidence thresholds or routing logic need adjustment.

Notice the progressive sophistication from simple patterns which require simple controls to complex architectures which demand mature operational practices. It's important to match your operational maturity to your architectural complexity.

## From MLOps to AgentOps

As AI systems have evolved from predictive models to autonomous agents that take real-world actions, the operational practices—the tools, processes, and frameworks teams use to deploy and maintain these systems—have had to evolve too. Each generation has brought challenges that existing practices couldn't handle.

### The Evolution of AI Ops

**MLOps (2010s)**: The set of processes and automation for managing data, code, and models to improve performance stability and long-term efficiency in ML systems. MLOps emerged as teams needed to move beyond experimental model training in notebooks to production-grade systems. It encompasses the entire ML lifecycle: data pipelines for feature engineering, model training and versioning, deployment infrastructure, monitoring for drift and performance degradation, and workflow orchestration to tie it all together. This requires unified governance for both data and models, scalable training infrastructure, CI/CD pipelines for model deployment, low latency model serving endpoints, and continuous monitoring to ensure models perform reliably over time.

**LLMOps (2020s)**: The practices and infrastructure required to tune, deploy, and manage large language models at scale. When models grew to billions of parameters, many teams faced new scale challenges: hosting models too large for single GPUs, managing distributed training across clusters, and controlling inference costs that could spiral out of control. LLMOps encompasses model optimization techniques like quantization and distillation, efficient tuning approaches using LoRA and QLoRA, prompt versioning and management, and safety measures to prevent harmful or biased outputs. Databricks provides managed infrastructure for tuning foundation models, optimized serving endpoints that handle batching and caching, cost controls to prevent budget overruns, and guardrails to ensure outputs meet safety and compliance standards.

**AgentOps (2024+)**: The practice and tooling to build, evaluate, deploy, govern, monitor, and maintain autonomous AI systems that can reason, plan, and take actions. Unlike LLMs that operate in single request-response cycles, agents execute multi-step workflows using tools to interact with real systems—querying databases, calling APIs, modifying files, and executing code. This shift from single-step inference to active tool use introduced entirely new operational challenges: establishing permission boundaries for tools, tracing complex multi-step reasoning chains, preventing cascading failures when agents make mistakes, handling retry logic and error recovery, and orchestrating multiple agents without conflicts or infinite loops.

Databricks addresses these challenges through Agent Bricks, a managed platform that automatically builds, optimizes, and deploys AI agents—handling model selection, fine-tuning with proprietary data, evaluation, monitoring, and continuous optimization behind the scenes. Databricks also provides capabilities for DIY, code-first agent development: evaluation frameworks to test agent behavior before deployment, comprehensive tracing for debugging multi-step decisions, secure tool execution with permission controls, monitoring for cost and performance, and serving infrastructure that scales to zero when idle.

### Core AgentOps Challenges

Building on lessons from MLOps and LLMOps, AgentOps must address five key operational challenges:

1. **Reliability**: Ensuring agents perform consistently across diverse inputs and edge cases
2. **Observability**: Making every decision point and tool call traceable and explainable. This includes verifying that the correct tools are being called at the right times, and that the LLM is appropriately balancing the flexibility provided by its probabilistic nature and ability to understand semantic intent with deterministic functionality to minimize hallucinations and errors.

3. **Safety**: Establishing boundaries to prevent harmful or unintended actions
4. **Scalability**: Supporting multiple concurrent agents without resource conflicts
5. **Maintainability**: Updating agent behaviors without breaking existing workflows

## Leveraging Proprietary Data

While prompt engineering and model selection are important, the ability to integrate and leverage proprietary, domain-specific data is what transforms agents from interesting demos into production-grade systems that deliver real business value. Without access to an organization's unique data and context, even the most sophisticated agents remain generic tools that struggle to address specific problems and use cases.

Organizations that successfully integrate proprietary data into their agent systems gain differentiated capabilities that understand specific domains better than any generic solution, operational efficiency through reduced need for human intervention in routine tasks, continuous improvement by learning from unique data patterns and use cases, and a strategic moat where proprietary data becomes a defensible competitive advantage.

### Why Generic LLMs Aren't Enough

Public, general-purpose LLMs excel at high level reasoning and broad knowledge tasks, but enterprise agent deployments require:

- **Domain expertise**: Industry-specific terminology, best practices, and regulations
- **Organizational context**: Internal processes, policies, and historical decisions
- **Real-time information**: Current inventory, live metrics, and up-to-date documentation
- **Proprietary insights**: Customer data, trade secrets, and competitive intelligence

### Common Data Integration Patterns

Two fundamental approaches exist for integrating proprietary data into agent systems: retrieval, which accesses external data at runtime through tools, and fine-tuning, which embeds knowledge directly into model weights.

#### Retrieval

Retrieval enables agents to access external data during inference, keeping responses current without model retraining. The simplest retrieval pattern is RAG (Retrieval-Augmented Generation), which follows a fixed sequence: query the data source, retrieve relevant information, augment the prompt with this context, and generate a response. More sophisticated agent architectures use dynamic retrieval, where the agent reasons about which data sources to query, what information to retrieve, and how to combine results from multiple sources.

##### Data Sources for Retrieval

**Vector Databases**: Store embedded documents for semantic search, enabling agents to find conceptually similar information even when exact keywords don't match. Vector databases excel at identifying relevant documents within large stores of unstructured data. Databricks Vector Search is a native vector database integrated with Delta Lake on Databricks.

**Structured Data Systems**: Enable SQL and structured queries against relational databases, NoSQL stores, data warehouses, and data lake formats like Delta and Iceberg tables. Agents can query these systems through tools like Databricks SQL or Lakebase.

**APIs**: Provide real-time data from internal services and external providers. Agents call these APIs to fetch current prices, check inventory levels, retrieve user information, or trigger actions in other systems. Protocols like MCP (Model Context Protocol) enable agents to discover available data APIs and call them correctly and performantly.

**Knowledge Graphs**: Represent entities and their relationships, allowing agents to traverse connections and reason about complex dependencies. Graph databases enable queries that would be inefficient in traditional databases, such as finding all entities within N degrees of separation.

**Files and Objects**: Direct access to documents, images, PDFs, and other unstructured data stored in object storage (S3, Azure Blob), Volumes in Databricks Unity Catalog, file systems, or content management systems. Agents can parse these files on-demand to extract specific information.

#### Fine-tuning and Adaptation

Fine-tuning embeds domain-specific knowledge directly into model weights, eliminating the need to retrieve or include this information in every prompt. This approach works well for fundamental domain knowledge that rarely changes—industry terminology, company-specific conventions, or specialized reasoning patterns. Organizations typically fine-tune open-source models using techniques like LoRA (Low-Rank Adaptation) or QLoRA, which reduce computational costs by training only a small subset of parameters. Databricks Mosaic AI Fine Tuning provide managed infrastructure for fine-tuning LLMs on proprietary data.

### Operational Complexity of Data Integration

Integrating proprietary data transforms agents from generic tools into domain specific systems, but introduces operational challenges that require active management:

**Data freshness** becomes critical when agents rely on current information. Customer records update, documentation changes, and inventory shifts; if your agent retrieves stale data, it provides incorrect answers. This requires automated indexing pipelines that detect changes, update triggers that refresh embeddings in real-time, cache invalidation strategies, and monitoring to alert when data becomes outdated.

**Access control** must enforce that agents only access data appropriate for each user's permissions. A support agent querying customer data shouldn't see financial records, and agents serving external users shouldn't access internal documents. This demands row-level security policies, attribute-based access control (ABAC) that considers user attributes and data sensitivity, integration with corporate identity providers, and comprehensive audit logging.

**Quality assurance** directly impacts agent reliability. Poor data quality like inconsistent formatting, missing fields or outdated information, leads to poor agent outputs. Teams need data validation frameworks to catch quality issues before indexing, schema evolution management as data structures change, versioning of data sources to track what data was used for which agent version, and regression testing to verify agents still work correctly when data changes.

**Retrieval performance** affects user experience and agent capability. Searching millions of documents in real-time requires optimized indexes (often vector indexes for semantic search), strategic caching of frequently-accessed data, query performance monitoring to detect slow retrievals, and sometimes pre-computing embeddings or summaries to reduce runtime overhead.

**Compliance** constrains how data can be used in agent systems. Regulations like GDPR, HIPAA, and industry-specific requirements mandate data lineage tracking (knowing exactly which data influenced which agent response), PII detection and redaction to prevent leaking sensitive information, geographic data residency controls, retention policies that govern how long data can be stored, and comprehensive audit trails for regulatory reporting.

**Observability** becomes increasingly complex as agents interact with multiple data sources. Teams need comprehensive tracing to understand which data sources influenced each agent decision, how retrieval quality varies across different contexts, and where data-related failures occur in multi-step workflows. This visibility is essential for debugging issues and optimizing data integration strategies.

These challenges compound when combining multiple data sources or supporting multiple agents accessing shared data. A mature AgentOps practice treats data integration not as a one-time setup task, but as an ongoing operational concern requiring dedicated tooling, monitoring, and governance processes.

## Agent Ops Anti-patterns

Building effective agent systems requires avoiding common architectural and operational mistakes that seem reasonable in theory but create problems in practice. These anti-patterns typically stem from overengineering solutions, ignoring simpler alternatives, or failing to implement proper operational controls. The following patterns represent lessons learned from production deployments.

### Starting Too Broad
Broad, undefined scopes lead to unclear success criteria, difficult evaluation, and agents that perform poorly across all domains rather than excelling in one. Start narrow, prove value, then expand deliberately.

❌ **Bad**: "Build an internal chatbot that answers every possible employee question"

✅ **Good**: "Build an agent that helps with IT support ticket triage"

### Over-Complex Architecture
Multi-agent systems introduce orchestration overhead, debugging complexity, and potential infinite loops between agents. Use them only when you need parallel execution, role specialization, or truly independent decision-making. If your workflow is predictable and sequential, a chain is simpler, faster, and more reliable.

❌ **Bad**: Supervisor agents managing sub-agents when a simple chain would work

```{mermaid}
graph TD
    U[User Query] --> S[Supervisor Agent]
    S --> A1[Research Agent]
    S --> A2[Analysis Agent] 
    S --> A3[Writing Agent]
    A1 --> S
    A2 --> S
    A3 --> S
    S --> R[Response]
    
    style S fill:#ffcdd2
    class S bad
```

✅ **Good**: Deterministic chain for predictable workflows

```{mermaid}
graph LR
    U[User Query] --> R[Research] --> A[Analyze] --> W[Write] --> Resp[Response]
    
    style R,A,W fill:#c8e6c9
```

### ReAct Loop for Single-Tool Tasks
ReAct (Reason-Act-Observe) loops add latency and cost for tasks where the action is predetermined. Reserve reasoning loops for situations with genuine uncertainty about next steps.

❌ **Bad**: Using a Reason→Act loop when one deterministic call would do

```text
LLM: "Thinking..."
→ Tool: getOrderStatus(order_id)
← LLM: "Thinking more..."
→ Tool: getOrderStatus(order_id) (again)
```

✅ **Good**: Direct, parameterized function

```python
status = get_order_status(order_id)  
return format_status(status)
```

### Overcomplicated Tools
Text-to-SQL and other LLM-powered tools introduce latency, unpredictability, and potential security risks when a parameterized query would suffice. Use LLMs to generate or select tool parameters only when the mapping from user intent to parameters genuinely requires reasoning.

❌ **Bad**: e.g., Text-to-SQL when you always need the same query with a single filter field.

✅ **Good**: Parameterized or stored query with allowlisted inputs

### Unoptimized Retrieval
Retrieval quality directly impacts agent performance, but teams often accept the first RAG implementation without measuring or optimizing it. Systematic measurement of retrieval relevance, groundedness, and citation quality reveals opportunities to dramatically improve results through better chunking, hybrid search, or reranking.

❌ **Bad**: Single dense index, oversized chunks, no filters or reranking

✅ **Good**:

- Apply metadata filters
- Right-size chunks and overlap
- Consider hybrid search 
- Add reranking
- Evaluate retrieval relevance, groundedness and citation coverage regularly

### Ungoverned Tools
Agents with unrestricted tool access create security, compliance, and reliability risks. Tools need explicit boundaries: what they can access, how long they can run, and how frequently they can be called. Without these controls, a single misbehaving agent can cascade into system-wide failures.

❌ **Bad**: Agents can call any endpoint or run arbitrary code with broad credentials

✅ **Good**:

- Tool registry with explicit schemas and allowlists
- Per-tool permissions, timeouts, and rate limits
- Audit logs and PII redaction for inputs/outputs

### No Rollback / Versioning
Agent behavior emerges from the interaction of multiple components—prompts, tools, routing logic, and data sources. Without versioning and the ability to roll back to known-good configurations, debugging production issues becomes nearly impossible. Treat agent configurations as code: version everything, deploy incrementally, and maintain rollback capabilities.

❌ **Bad**: Shipping unlogged agents you can’t reproduce

✅ **Good**:

- Version prompts, tools, and routing rules; maintain an agent registry
- Promote via dev -> staging -> prod with canary/shadow options
- Keep a rollback path when metrics regress

### No Systematic Evaluation
Manual testing catches obvious errors but misses edge cases and doesn't scale. In production, agents encounter diverse inputs that expose failure modes invisible during development. Systematic evaluation enables the observability needed to understand whether the right tools are being called, if retrieval is returning relevant context, when agents are hallucinating, and where bottlenecks occur. Without granular measurement, teams lack the context to root cause issues and make data-driven decisions. For domain-specific applications, integrate subject matter experts early to ensure evaluation criteria reflect real world requirements.

❌ **Bad**: Playing whack-a-mole with errors as they appear

✅ **Good**: Comprehensive error characterization and test suites

### No Human-in-the-Loop for High-Stakes Decisions

Agents that autonomously execute high-stakes actions without human oversight create unacceptable risk. Even well tested agents can misinterpret edge cases or encounter novel situations. Critical decisions require human judgment, and agents need clear escalation paths when confidence is low or stakes are high.

❌ **Bad**: Agent autonomously processes refunds, cancels subscriptions, or modifies accounts without approval

✅ **Good**: Agent proposes actions for human approval based on:
- Transaction value thresholds (e.g., >$1000)
- Low confidence scores (e.g., <0.8)
- Sensitive operations (deletions, policy exceptions)
- Customer escalation requests

### No Cost Controls
   Agents without per-request budgets or rate limits can generate runaway costs through expensive tool calls, excessive LLM iterations, or unbounded retrieval. A single poorly-formed query or reasoning loop can consume an entire monthly budget.
   
   ❌ **Bad**: Agents with unlimited tool calls and no cost tracking
   
   ✅ **Good**:
   - Per-request token budgets and max iterations
   - Cost monitoring and alerts by agent/user
   - Circuit breakers for expensive operations

## Next Steps

In the following chapters, we'll explore the reference architecture for Agent Ops systems and learn how to build robust, production-ready agent deployments.
