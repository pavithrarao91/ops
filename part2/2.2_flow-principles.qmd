# Principle 1: The Principle of Flow
As outlined in The DevOps Handbook, teams must decrease deployment time for changes while increasing service reliability and quality. this principle remains foundational for generative AI systems. However, the generative AI introduces unique challenges

[*"Our goal is to decrease the amount of time required for changes to be deployed into production and to increase the reliability and quality of those services."* ](**The DevOps Handbook: How to Create World-Class Agility, Reliability, & Security in Technology Organizations eBook : Kim bestselling author of The, Gene, Humble, Jez, Debois, Patrick, Willis, John, Forsgren, Nicole: Amazon.co.uk: Books. (n.d.). https://www.amazon.co.uk/DevOps-Handbook-World-Class-Reliability-Organizations-ebook/dp/B0F7J1WWQD?ref_=ast_author_mpb**)

## Implementing Efficient Flows

### Creating Quick Feedback Loops Through Systematic Test Suite Development
Unit testing and integration testing in software engineering can usually be defined upfront because we have an idea of the behaviour of the system we want to build, including it's edge cases. Generative AI is different. Creating tests needs an additional preliminary step of  extensively analysing traces of previous requests and responses in order to identify common failures modes. Only when these failure modes are identified and categorised can we then defining automated checks to identify those failure patterns. 

**Dangerous default** 

Teams often fall into a dangerous default pattern where one tester runs five to six queries repeatedly in a chat interface and provides pass/fail analysis. This approach fails to capture error frequency metrics or classify failure types systematically.

**Teams should instead apply the scientific method iteratively.**

1. Start by categorizing failure modes as quickly as possible using approximately 100 traces. ([source](**Husain, H., & Shankar, S. (n.d.). Frequently Asked Questions (And Answers) about AI Evals – Hamel’s blog. Hamel’s Blog. https://hamel.dev/blog/posts/evals-faq/**)). These traces can be collected in a variety of ways:

    - If an application is already in production, sample production traces. Make sure production traces are sufficiently diverse and cover a range of input categories, conversation lengths and types of tool calls

    - If an application is still in development, work with an SME to define a set of inputs that they anticipate should cover the range of scenarios a workflow or subagent should cover. Because LLM calls are non-deterministic, each input should be run 3-4 times so we can measure response consistency. 

2. Create failure reports using tools like MLflow or Jupyter notebooks with visualizations [1](**Husain, H., & Shankar, S. (n.d.). Frequently Asked Questions (And Answers) about AI Evals – Hamel’s blog. Hamel’s Blog. https://hamel.dev/blog/posts/evals-faq/**).

3. Gradually calibrate automated LLM judges metrics based on insights from step 2 and consultation with domain experts. (We will cover a basic workflow in more detail the following chapter - 2.3 Feedback Principles, using a worked example of a customer email generator). The output will be a set of scorers such as [MLflow Gen AI Scorers](https://mlflow.org/docs/latest/genai/concepts/scorers/) that can scale up trace evaluation in both development and production settings. 

4. Setup scorers to run in both offline development mode, and in an online setting on production traces. For offline settings, this process involves passing an MLflow Dataset or Pandas Dataframe to the [`mlflow.genai.evaluate()`](https://docs.databricks.com/gcp/en/mlflow3/genai/eval-monitor/evaluate-app#step-4-run-evaluation-with-predefined-scorers) function. For online settings, this involves collecting traces into an OLAP system such as Delta tables, and then running batch jobs to evaluate a sample of these production traces using the same MLflow scorers.

5. Surface aggregated metrics in dashboards to monitor trends over time. Periodically have product managers / business SMEs review and refine test suites based on new failure modes observed in production.

It can be useful to start with failure reports that come readily available from existing evaluation frameworks. For example, the evaluations UI from an MLflow Experiment allows users to filter for traces that fail a specific assessment. These traces, along with the LLM judge rationale that led to the failed assessment, can be surfaced in a notebook, SQL Dashboard or Databricks Apps UI.

![failure-report-simple](./feedback-principles_files/failure-technical-report-03.png)

Once developers understand their failure reporting requirements in more detail, customised reports can also be created. For example, a summary (possibly LLM-generated) of the most common types of agent requests received can be included 

![failure-report-custom-01](./feedback-principles_files/failure-technical-report-01.png)

A statistical summary of tool calls is another section that can be included 
![failure-report-custom-02](./feedback-principles_files/failure-technical-report-02.png)

It's worth noting that seasoned traditional software developers often recommend "writing tests first". However, writing a full test suite upfront is rarely feasible with Generative AI. Unlike traditional software engineering, where we can reasonably anticipate the range of failure modes and edge cases, LLM outputs are less predictable in their outputs. Moreover, LLM capabilities differ across models and are constantly evolving with each new model release. Therefore, it's more often the case that edge cases have to be "discovered" and codified with empirical methods.

### The Six-Step Agent Development Lifecycle (Evaluation-Driven)

Generative AI blurs the lines between development and evaluation. After initial prototyping and “vibe checking,” evaluation should drive development priorities. LLM judges and SME reviews become the quantitative and qualitative engines that select where to invest engineering time, and they build trust with stakeholders over time.

The steps below assume initial project steps have been completed, including use case identification, team creation, and metric development for the evaluation workflow.

1. Prepare data and create tools for the agent
   - Inventory and clean source data, define access patterns, and establish privacy/PII handling.
   - Create tooling including [Unity Catalog Functions](https://docs.databricks.com/aws/en/udf/unity-catalog), [MCP Servers](https://docs.databricks.com/aws/en/generative-ai/mcp/), [Vector Search indices](https://docs.databricks.com/aws/en/generative-ai/vector-search), or [Genie Spaces](https://docs.databricks.com/aws/en/genie/)

2. Build a prototype; “vibe check” quality and debug with [MLflow traces](https://docs.databricks.com/aws/en/mlflow3/genai/tracing)
   - Ship the simplest walking skeleton; enable MLflow tracing which gives end-to-end observability for GenAI applications
   - Experiment with managed [Agent Bricks](https://www.databricks.com/product/artificial-intelligence/agent-bricks) to quickly create high quality agents via a simple UI.
   - Alternatively for a code-based approach leverage the [AI Playground](https://docs.databricks.com/aws/en/large-language-models/ai-playground) to experiment and make quick changes to the agent, then export to a code-based approach via the [Agent Framework](https://docs.databricks.com/aws/en/generative-ai/agent-framework/author-agent)

3. Create datasets to establish a benchmark for quality evaluation
   - Curate golden sets with [synthetic data](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/synthesize-evaluation-set) or beta testing from developers and SMEs
   - Refine rubrics and acceptance thresholds with SMEs/Product.
   - Calibrate [LLM judges](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/custom-judge/) and programmatic checks to score traces consistently.

4. Iteratively identify and fix root causes of quality issues
   - Use judge results and SME feedback via the [Review App](https://docs.databricks.com/aws/en/mlflow3/genai/human-feedback/concepts/review-app) to prioritize changes (prompt/tool design, retrieval, caching, routing).
   - Require evaluation deltas for every change; maintain a decision log and rollback plan.
   - Run a CI evaluation suite on every change and block merges on regressions versus baseline. Push to staging and run smoke tests before stakeholder sign-off.

5. Get sign-off on pre-production agents from stakeholders
   - Review thresholds, costs, and risks in a structured demo; confirm guardrails and SLOs.
   - Create [dashboards](https://docs.databricks.com/aws/en/ai-bi/) for regressions and cost spikes.

6. Release to production and monitor quality
   - Leverage [AI Gateway](https://www.databricks.com/product/artificial-intelligence/ai-gateway) to enforce guardrails, establish fallbacks and usage tracking.
   - Use staged rollout (feature flags or percentage-based canaries) with defined rollback triggers based on guardrail/judge deltas.
   - Run evaluations on [production traces](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/production-monitoring).
   - Close the loop with SMEs and end users; feed new edge cases back into datasets and judges.

In short, development and evaluation quickly become the same loop. After the early “vibe check,” let LLM judges and SME reviews decide where to invest engineering time next. Judges give you repeatable numbers; SMEs bring grounded judgment—turning evaluation into a quantitative roadmap for product decisions and, just as important, a transparent basis for building stakeholder trust.

### Proactively Manage Evaluation Costs

Unlike traditional unit tests, LLM-as-a-judge calls cost money. Agent traces can span significant lengths, increasing token costs per test. Additionally, production monitoring entails running evaluation suites on a recurring basis, which can increase costs even further. Therefore, to ensure fast and continuous feedback, cost optimization is unavoidable. Teams need strategies for creating fast and affordable evaluation suites. A non-exhaustive list is included below.

**1. Reduce tokens sent to LLMs.**

Rather than sending entire traces, experiment with strategies for reducing the input tokens sent to an LLM. 

Some strategies include: 

- Test more efficient text formatting approaches. For example, replace JSON formatting with XML or free text alternatives. 
- Create tests are target specific inputs, outputs and expectations. As of MLflow version 3.3.3, the MLflow [`make_judge`](https://mlflow.org/docs/3.4.0/genai/eval-monitor/scorers/llm-judge/make-judge/) API supports this in two ways:
    - injecting only specific inputs, outputs and expections into the evaluation prompt to LLM judge assessment. (Field-based evaluation)
    - passing an entire trace to an intelligent agent that fetches and analyzes the trace data, allowing it to focus on relevant aspects based on your evaluation instructions (Trace-based evaluation). This design enables trace-based judges to handle large, complex execution flows without hitting token limits.

*Example of field-based evaluation in MLflow*
```
from mlflow.genai.judges import make_judge

# Create a toy agent that responds to questions
def my_agent(question):
    # Simple toy agent that echoes back
    return f"You asked about: {question}"


# Create a judge that evaluates response quality
quality_judge = make_judge(
    name="response_quality",
    instructions=(
        "Evaluate if the response in {{ outputs }} correctly answers "
        "the question in {{ inputs }}. The response should be accurate, "
        "complete, and professional."
    ),
    model="openai:/gpt-4",
)

# Get agent response
question = "What is machine learning?"
response = my_agent(question)

# Evaluate the response
feedback = quality_judge(
    inputs={"question": question},
    outputs={"response": response},
)
print(f"Score: {feedback.value}")
print(f"Rationale: {feedback.rationale}")
```

### Example: GitHub Actions CI gate with Databricks Agent Evaluation

To run evaluations automatically in CI, store repository secrets `DATABRICKS_HOST`, `DATABRICKS_TOKEN`, and `DATABRICKS_EVAL_JOB_ID`. The workflow below triggers a Databricks Job that runs your evaluation notebook, polls for completion, posts a formatted summary to the PR, and blocks merges when judges fail by requiring a manual-review environment.

```yaml
name: run-databricks-evaluation
on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  run-databricks-evaluation:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Trigger Databricks Evaluation Job
        id: trigger-job
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          DATABRICKS_JOB_ID: ${{ secrets.DATABRICKS_EVAL_JOB_ID }}
          GITHUB_PR_NUMBER: ${{ github.event.pull_request.number }}
          GITHUB_HEAD_REF: ${{ github.head_ref }}
        run: python .github/scripts/trigger_databricks_job.py

      - name: Wait for Job Completion and Get Results
        id: get-results
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          RUN_ID: ${{ steps.trigger-job.outputs.run_id }}
        run: python .github/scripts/get_job_results.py

      - name: Format Evaluation Results
        id: format-results
        run: python .github/scripts/format_results.py

      - name: Summarize Results
        if: always()
        run: |
          if [ -f formatted_results.md ]; then
            cat formatted_results.md >> "$GITHUB_STEP_SUMMARY"
          fi

    outputs:
      all-passed: ${{ steps.get-results.outputs.all-passed }}

  manual-review:
    needs: run-databricks-evaluation
    runs-on: ubuntu-latest
    if: ${{ needs.run-databricks-evaluation.outputs.all-passed != 'true' }}
    environment: production-approval
    steps:
      - name: Request Manual Review
        run: |
          echo "⚠️ Evaluation did not pass all checks automatically."
          echo "Manual review required. See PR summary for details."
```

*Example of trace-based evaluation in MLflow*

```
from mlflow.genai.judges import make_judge
import mlflow

# Create a more complex toy agent with tracing
@mlflow.trace
def my_complex_agent(query):
    with mlflow.start_span("parse_query") as parse_span:
        # Parse the user query
        parsed = f"Parsed: {query}"
        parse_span.set_inputs({"query": query})
        parse_span.set_outputs({"parsed": parsed})

    with mlflow.start_span("generate_response") as gen_span:
        # Generate response
        response = f"Response to: {parsed}"
        gen_span.set_inputs({"parsed": parsed})
        gen_span.set_outputs({"response": response})

    return response


# Create a judge that analyzes complete execution flows
trace_judge = make_judge(
    name="agent_performance",
    instructions=(
        "Analyze the {{ trace }} to evaluate the agent's performance.\n\n"
        "Check for:\n"
        "1. Efficient execution and tool usage\n"
        "2. Error handling and recovery\n"
        "3. Logical reasoning flow\n"
        "4. Performance bottlenecks\n\n"
        "Provide a rating: 'excellent', 'good', or 'needs improvement'"
    ),
    model="openai:/gpt-4",  # Note: Cannot use 'databricks' model
)

# Execute agent and capture trace
with mlflow.start_span("agent_task") as span:
    response = my_complex_agent("What is MLflow?")
    trace_id = span.request_id

# Get the trace
trace = mlflow.get_trace(trace_id)

# Evaluate the trace
feedback = trace_judge(trace=trace)
print(f"Performance: {feedback.value}")
print(f"Analysis: {feedback.rationale}")
```

- Use non-LLM-based checks when possible.
Deterministic tests that compare received and expected outputs in a rule-based way is both faster and cheaper than using an LLM. For example, use keyword matching to identify whether a customer support ticket issue classifier returns one of a predefined list of categories. This approach eliminates inference costs entirely for certain test categories.

- Separate evaluation datasets into distinct classes oftest suites.
CI tests can be run quickly by limiting the number of input examples to a small number, for example approximately 100 examples. The main goal is to identify regressions before production deployment. 

Production test suites can run on a larger sample of live traces through an offline batch job. The goal here is different to that of CI testing. Here, we are less time-constrained, and we want a large enough sample to allow us to identify new, uncategorized errors for triage.


### Enabling Easy Rollbacks During Failures
Teams need robust rollback capabilities when agent or tool versions fail. When issues are detected — whether from monitoring, failed tests, or user feedback — immediate rollbacks help restore service and keep the value stream moving. With generative AI, rollbacks don't only involve reverting back to a previous application version. There are other aspects of versioning to consider such as prompt versions, code versions and agent endpoint versions. 

Artifact versioning can be managed in Databricks through various features: 

- MLflow model versions in the MLflow model registry enable quick version management. 
- API endpoints can revert to previous model versions instantly. 
- Prompt registries maintain version control for prompt iterations. 
- Central repositories should store tested tools with proper versioning to ensure reliable rollback capabilities.

#### Example: ML Model Lifecycle Workflows in Unity Catalog

**1. Before Deployment: Add Model Metadata**

- Track data lineage using `mlflow.log_input` to link model to training datasets
- Add descriptions, tags, and version-specific information through UI or API

**2. Create Champion-Challenger Model Setup**

*2.1: Create Challenger Model*

- Register new model version using same process as above
- Version numbers are automatically incremented (e.g., version 1, 2, 3...)

*2.2 Set Model Aliases*

- Assign "Champion" alias to current production model and "Challenger" alias to new version:
```python
from mlflow import MlflowClient
client = MlflowClient()

###### Set Champion (current production)
client.set_registered_model_alias("prod.ml_team.model_name", "Champion", 1)

###### Set Challenger (new candidate)
client.set_registered_model_alias("prod.ml_team.model_name", "Challenger", 2)
```

**3: Deploy for A/B Testing**

- Reference models by alias in inference workloads:
```python
# Load Champion model
champion_model = mlflow.pyfunc.load_model("models:/prod.ml_team.model_name@Champion")

# Load Challenger model
challenger_model = mlflow.pyfunc.load_model("models:/prod.ml_team.model_name@Challenger")
```

**4. Promote Challenger to Champion**

- After validation, reassign aliases:
```python
# Promote Challenger to Champion
client.set_registered_model_alias("prod.ml_team.model_name", "Champion", 2)

# Optionally retire old Champion
client.delete_registered_model_alias("prod.ml_team.model_name", "Challenger")
```

**5. Model Rollback Process**

*5.1: Identify Rollback Target*

- List available model versions:
```python
client = MlflowClient()
versions = client.search_model_versions("name='prod.ml_team.model_name'")
```

*5.2 Quick Rollback Using Aliases*
- Reassign "Champion" alias to previous stable version:
```python
# Rollback Champion alias to version 1
client.set_registered_model_alias("prod.ml_team.model_name", "Champion", 1)
```

*5.3 Update Deployment Systems*
- Batch inference workloads automatically pick up the rollback on next execution when using aliases
- For model serving endpoints, update via REST API:
```python
champion_version = client.get_model_version_by_alias("prod.ml_team.model_name", "Champion")
# Update serving endpoint with rollback version
```

**Step 4: Verify Rollback**
- Test rolled-back model in staging environment
- Monitor performance metrics to confirm successful rollback
- Document rollback reason and actions taken

This workflow ensures safe model deployment with quick rollback capabilities while maintaining full governance and traceability.

# Further reading:

- [Optimizing LLM prompts for low latency](_Optimizing LLM prompts for low latency | Building with AI_. (n.d.). incident.io. https://incident.io/building-with-ai/optimizing-llm-prompts#case-study-planning-grafana-dashboards-for-an-incident) 
- [From Noob to Automated Evals In A Week (as a PM) w/Teresa Torres](Hamel Husain. (2025, August 15). _From Noob to Automated Evals In A Week (as a PM) w/Teresa Torres_ [Video]. YouTube. https://www.youtube.com/watch?v=N-qAOv_PNPc)
- [LLM Inference Performance Engineering: Best Practices](_LLM Inference Performance Engineering: Best Practices | DataBricks blog_. (n.d.). Databricks. https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)

